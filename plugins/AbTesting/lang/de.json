{
    "AbTesting": {
        "NExperiments": "%s A\/B Tests",
        "ColumnConversionsPerVisit": "Konversionen pro Besuch",
        "ColumnRevenuePerVisit": "Umsatz pro Besuch",
        "ColumnOrdersPerVisit": "Bestellungen pro Besuch",
        "ColumnOrdersRevenuePerVisit": "Umsatz pro Besuch (von Bestellungen)",
        "ColumnDetectedEffect": "Erkannte Auswirkung",
        "ColumnDetectedEffectDocumentation": "Der Anteil der Veränderung verglichen mit der originalen Version. Ein positiver erkannter Effekt bedeutet, dass diese Variante besser funktioniert als die originale Version während eine negative erkannte Auswirkung bedeutet, dass die Variante schlechter funktioniert als die originale Version.",
        "ColumnRemainingVisitors": "Verbleibende Besucher",
        "ColumnRemainingVisitorsDocumentation": "Die Anzahl der zusätzlichen Besucher die benötigt werden, um ein bestandskräftiges Resultat zu schließen.",
        "ColumnSignificanceRate": "Statistische Signifikanz",
        "ColumnSignificanceRateDocumentation": "Je höher die statistische Signifikanz ist, umso höher ist die Wahrscheinlichkeit, dass die erzielte Auswirkung auch echt und wiederholbar ist und nicht zufällig erzielt wurde.",
        "ColumnTimeOnSite": "Zeit auf der Website",
        "ColumnTotalConversions": "Gesamtanzahl der Konversionen",
        "ColumnTotalConversionsPerVisit": "Konversionen pro Besuch",
        "ColumnTotalRevenue": "Gesamtumsatz",
        "ColumnTotalRevenuePerVisit": "Umsatz pro Besuch",
        "ColumnVisitsDocumentation": "Die Anzahl der Besuche die am A\/B Test teil genommen haben. Alle Besuche die am A\/B Test teil genommenen haben zählen, daher ist diese Anzahl für gewöhnlich höher als die Anzahl der \"aktiv teil genommenen Besuche\".",
        "ColumnVisitsEnteredDocumentation": "Die Anzahl der Besuche wo der Besucher eine Ziel-Seite dieses A\/B Tests besucht hat.",
        "ColumnUniqueVisitorsDocumentation": "Die Anzahl der eindeutigen Besucher die an diesem A\/B Test teil genommen haben.",
        "ColumnBounceRateDocumentation": "Der Anteil der Besuche die nur eine Seite besuchten und am A\/B Test teil genommen haben. Dies bedeutet, dass der Besucher direkt eine Ziel-Seite des A\/B Tests wieder verlassen hat, nachdem er dem A\/B Test beigetreten ist.",
        "ColumnBouncesDocumentation": "Die Anzahl der Besuche die nur eine Seite besuchten und am A\/B Test teil genommen haben. Dies bedeutet, dass der Besucher direkt eine Ziel-Seite des A\/B Tests wieder verlassen hat, nachdem er dem A\/B Test beigetreten ist.",
        "ChooseExperiment": "Wähle A\/B Test",
        "EcommerceOrders": "E-Commerce Bestellungen",
        "EcommerceOrdersRevenue": "E-Commerce Bestellungen Umsatz",
        "ConclusionNoVariationRecordedYet": "Bisher wurden noch keine A\/B Test-Daten aufgezeichnet. Es erscheint, als hat noch kein Besucher am A\/B Test teil genommen. Vielleicht muss der Code für den A\/B Test noch in Ihr Projekt eingebunden werden.",
        "ConclusionWinningVariation": "Es existiert eine Variante, die signifikant besser als die originale Version funktioniert und es übertrifft die erwartete Minimal Erkannte Auswirkung von %1$s.",
        "ConclusionSignificantVariation": "Es existiert eine Variante, die signifikant besser als die originale Version funktioniert aber es übertrifft nicht die erwartete Minimal Erkannte Auswirkung von %1$s.",
        "ConclusionLosingVariation": "Es existiert eine Variante, die signifikant schlechter als die originale Version funktioniert.",
        "ConclusionNoVariationHasEnoughVisitors": "Mehr Besucher werden benötigt, um ein bestandskräftiges Resultat zu schließen.",
        "ConclusionNoVariationIsSignificant": "Keine Variante hat derzeit genug Signifikanz um ein bestandskräftiges Resultat zu schließen.",
        "ConclusionNoConclusion": "Um ein bestandskräftiges Resultat zu schließen, darf eine Variante keine verbleibenden Besucher benötigen und es muss statistisch signifikant sein.",
        "NameOriginalVariation": "Original",
        "ErrorCreateNoUrlDefined": "Bitte geben Sie eine URL an, um zu Bestimmen auf welchen Seiten der A\/B Test aktiviert werden soll.",
        "ErrorXNotProvided": "Bitte geben Sie einen Wert für \"%1$s\" an.",
        "ErrorXTooLong": "\"%1$s\" ist zu lang, maximal %2$s Zeichen sind erlaubt.",
        "ErrorXTooLow": "\"%1$s\" ist zu lang, der Wert muss mindestens %2$s sein.",
        "ErrorXTooHigh": "\"%1$s\" ist zu niedrig, der Wert darf maximal %2$s sein.",
        "ErrorXNotANumber": "\"%1$s\" muss eine Nummer sein.",
        "ErrorXNotWhitelisted": "Der Wert für \"%1$s\" ist nicht erlaubt, verwenden Sie einen von: %2$s.",
        "ErrorXContainsWhitespace": "Der \"%1$s\" darf keine Leerzeichen enthalten.",
        "ErrorXContainsOnlyNumbers": "Der \"%1$s\" darf nicht nur aus Zahlen bestehen, bitte verwenden Sie zumindest einen Buchstaben.",
        "ErrorXOnlyAlNumDash": "Sonderzeichen für \"%1$s\" sind nicht erlaubt.",
        "ErrorXNotInFuture": "\"%1$s\" muss in der Zukunft sein.",
        "ErrorXLaterThanY": "\"%1$s\" ist später als \"%2$s\".",
        "ErrorXLaterThanYButEqual": "\"%1$s\" muss später sein als \"%2$s\" aber sie sind gleich.",
        "ErrorNotEnabledForExperiment": "Die angegebene \"%1$s\" ist nicht aktiviert für diesen A\/B Test.",
        "ErrorNotAnArray": "\"%1$s\" muss ein Array sein.",
        "ErrorInnerIsNotAnArray": "Jedes \"%1$s\" innerhalb von \"%2$s\" muss ein Array sein.",
        "ErrorArrayMissingKey": "Fehlender array Schlüssel \"%1$s\" in \"%2$s\" an der Position \"%3$s\".",
        "ErrorArrayMissingValue": "Fehlender Wert für den Array-Schlüssel \"%1$s\" in \"%2$s\" bei Position \"%3$s\".",
        "ErrorInvalidValue": "Ungültiger Wert für \"%1$s\" angegeben (\"%2$s\").",
        "ErrorExperimentDoesNotExist": "Der angeforderte A\/B Test existiert nicht",
        "ErrorExperimentNameIsAlreadyInUse": "Der Name des A\/B Tests wird bereits von einem anderen A\/B Test verwendet.",
        "ErrorExperimentAlreadyStarted": "Der A\/B Test kann nicht gestartet werden da es bereits gestartet wurde.",
        "ErrorExperimentCannotBeFinished": "Der A\/B Test kann nicht beendet werden da es bereits beendet wurde.",
        "ErrorExperimentCannotBeUpdatedBecauseArchived": "Dieser A\/B Test kann nicht aktualisiert werden da es bereits archiviert wurde.",
        "ErrorVariationNameOriginalNotAllowed": "Der Name \"Original\" ist ein reservierter Varianten-Name und kann nicht verwendet werden.",
        "ErrorInvalidRegExp": "Der reguläre Ausdruck \"%1$s\" hat kein gültiges Format.",
        "ErrorNotValidUrl": "%1$s ist keine valide URL. Stellen Sie sicher, dass die URL zum Beispiel mit http:\/\/ beginnt.",
        "FieldSuccessMetricsLabel": "Wähle eine oder mehrere Erfolgsmetriken",
        "FieldSuccessMetricsHelp1": "Erfolgsmetriken helfen Ihnen zu messen welche Varianten am erfolgreichsten sind und helfen Ihnen zu entscheiden, welche Variante in der Zukunft verwendet werden sollte.",
        "FieldSuccessMetricsHelp2": "Sie können eine oder mehrere Metriken auswählen, um Ihre Hypothese zu überprüfen. Matomo wird für jede ausgewählte Metrik einen Report anzeigen, damit Sie die verschiedenen Varianten miteinander vergleichen können.",
        "FieldIncludedTargetsLabel": "Ein Besucher nimmt am A\/B Test teil wenn",
        "FieldExcludedTargetsLabel": "Ein Besucher nimmt nicht am A\/B Test teil wenn",
        "FieldExcludedTargetsHelp": "Indem Sie bestimmte Seiten ausschließen, können Sie einschränken, auf welchen Seiten ein Besucher den A\/B Test nicht beitreten wird. Wenn für eine Seite nur eine Bedingung davon zutrifft, wird der A\/B Test für einen Besucher auf dieser Seite nicht aktiviert. Es müssen nicht alle Bedingungen zutreffen.",
        "FieldRedirectHelp2": "Falls die Umleitungen nicht auf allen Seiten ausgeführt werden sollen, stellen Sie bitter sicher, dass unter \"Ziel-Seiten\" definiert ist, wann eine Umleitung stattfinden soll.",
        "FieldRedirectHelp3": "Mit unserer %sPHP A\/B Tests Bibliothek%s können Sie in Ihrem PHP-Projekt Ihre Nutzer auch server-seitig umleiten.",
        "ClickToCreateNewGoal": "Klicken Sie hier um ein neues Ziel zu erstellen.",
        "FormScheduleIntroduction": "Standardmäßig startet ein A\/B Test sobald der A\/B Test in Ihr Projekt eingebunden ist und endet, sobald Sie es manuell beenden. Alternativ können Sie ein Start- und Enddatum terminieren.",
        "FieldScheduleExperimentStartLabel": "Starte A\/B Test an",
        "FieldScheduleExperimentStartHelp": "Lassen Sie das Feld leer wenn der A\/B Test automatisch gestartet werden soll, sobald der A\/B Test in Ihr Projekt eingebunden wurde. Stellen Sie sicher, dass der A\/B Test vor dem definierten Startdatum komplett konfiguriert ist. Es ist nicht empfohlen einen A\/B Test zu ändern, sobald es im Gange ist, da es zu Fehlinterpretationen führen kann. Das angegebene Datum wird in der Zeitzone %1$sUTC%2$s erwartet.",
        "FieldScheduleExperimentFinishHelp": "Lassen Sie das Feld leer, wenn Sie den A\/B Test manuell beenden möchten. Wenn Sie ein Datum definieren, wird der A\/B Test automatisch zu dieser Zeit beendet. Falls Sie ein Datum definieren, stellen Sie sicher, dass der A\/B Test lange genug laufen wird damit die Ergebnisse echt und nicht zufällig erzielt werden. Das angegebene Datum wird in der Zeitzone %1$sUTC%2$s erwartet.",
        "FieldScheduleExperimentFinishLabel": "Beende A\/B Test an",
        "FieldPercentageParticipantsLabel": "Anteil der Besucher die am A\/B Test teilnehmen werden",
        "FieldPercentageParticipantsHelp": "Definieren Sie wie viele Ihrer Besucher an diesem A\/B Test teilnehmen sollen. Wenn Sie 70% auswählen, dann werden 70% aller Ihrer Besucher an diesem A\/B Test teilnehmen und entweder die originale Version oder eine beliebige Variante davon sehen. Die anderen 30% werden nicht am A\/B Test teilnehmen und immer die originale Version sehen.",
        "FieldPercentageVariationsLabel": "Anteil Ihres Traffics der jeder Variante zugewiesen wird",
        "FieldPercentageVariationsHelp": "Es ist empfohlen jede Variante gleich oft zu aktivieren (Standard) aber Sie können den Anteil für jede Variante ändern. Die Summe aller Varianten inklusive der originalen Version sollte 100% betragen. Stellen Sie sicher, dass ein Anteil des Traffics der originalen Version zugewiesen ist.",
        "FieldVariationsHelp": "Variante ist die Bezeichnung für eine neue Version die Sie mit der originalen Version vergleichen. Wenn Sie zum Beispiel verschiedene Button-Farben miteinander vergleichen möchten, erstellen Sie eine Variante für jede Farbe die Sie vergleichen möchten. Verwenden Sie lediglich Buchstaben und Zahlen ohne Leerzeichen und Sonderzeichen. Maximal 50 Zeichen können für jede Variante verwendet werden.",
        "ErrorVariationAllocatedNot100Traffic": "Sie haben in der Summe mehr als 100% zugewiesen. Bitte definieren Sie einen niedrigeren Wert für eine oder mehrere Varianten damit die Summe insgesamt 100% ist.",
        "ErrorVariationAllocatedNotEnoughOriginal": "Die originale Version erhält wesentlich mehr Traffic als es standardmäßig würde. Es ist empfohlen den zugewiesenen Traffic für die originale Version zu erhöhen, indem Sie einen niedrigeren Anteil für die Varianten setzen.",
        "EqualsDateInYourTimezone": "Dies entspricht der folgenden Zeit in Ihrer Zeitzone:",
        "Filter": "Filter",
        "CurrentTimeInUTC": "Die aktuelle Zeit in UTC ist",
        "NoExperimentsFound": "Es existieren keine A\/B Tests mit diesem Status.",
        "DeleteExperimentInfo": "Lösche A\/B Test. Es wird nicht möglich sein den A\/B Test wiederherzustellen.",
        "ViewReportInfo": "Gehe zum Report für diesen A\/B Test.",
        "ArchiveReportInfo": "Archiviere A\/B Test. Wenn Sie einen A\/B Test archivieren, wird es nicht gelöscht, aber der A\/B Test wird nicht mehr im Reporting-Menü erscheinen und nicht mehr für die Segmentierung verfügbar sein.",
        "ArchiveReportConfirm": "Wollen Sie wirklich diesen A\/B Test archivieren? der A\/B Test wird nicht mehr im Reporting-Menü angezeigt und wird nicht mehr für die Segmentierung zur Verfügung stehen, sobald es archiviert wurde.",
        "DeleteExperimentConfirm": "Wollen Sie wirklich diesen A\/B Test löschen? Sobald ein A\/B Test gelöscht wurde, kann es nicht mehr wiederhergestellt werden.",
        "UrlParameterValueToMatchPlaceholder": "Wert für den URL-Parameter",
        "TargetPageTestTitle": "URL Prüfer",
        "TargetPageTestLabel": "Geben Sie eine URL inklusive Schema an um zu überprüfen, ob ein Besucher dem A\/B Test beitreten wird, wenn ein Besucher diese URL aufruft:",
        "TargetPageTestErrorInvalidUrl": "Geben Sie eine URL inklusive Schema an.",
        "TargetPageTestUrlMatches": "Ein Besucher wird diesen A\/B Test bei dieser URL beitreten",
        "TargetPageTestUrlNotMatches": "Ein Besucher wird diesen A\/B Test bei dieser URL nicht beitreten",
        "ExperimentCreatedInfo1": "dieser A\/B Test ist terminiert gestartet zu werden am",
        "ExperimentCreatedInfo2": "und wird laufen bis",
        "ExperimentCreatedInfo3": "Stellen Sie sicher alle Änderungen zu machen bevor der A\/B Test startet. Es ist nicht empfohlen einen laufenden A\/B Test zu verändern, sobald es gestartet wurde.",
        "ExperimentRunningInfo1": "dieser A\/B Test startete am",
        "ExperimentRunningInfo2": "und ist terminiert beendet zu werden am",
        "ExperimentRunningInfo3": "Es ist empfohlen, keine Änderungen vorzunehmen, wenn ein A\/B Test im Gange ist, da es dann zu Fehlinterpretationen mit dem Ergebnis kommen kann.",
        "ExperimentFinishedInfo1": "dieser A\/B Test ist beendet. Es ist empfohlen keine Änderungen vorzunehmen, wenn ein A\/B Test beendet wurde, da dies zu Fehlinterpretationen im Ergebnis führen kann.",
        "ExperimentFinishedInfo2": "Stellen Sie sicher, zuvor eingebundenen Code für diesen A\/B Test von Ihrer Website, Anwendung oder Server zu entfernen.",
        "RelatedActions": "Aktionen, die hier ausgeführt werden können",
        "ExperimentWillStartFromFirstTrackingRequest": "dieser A\/B Test startet automatisch, sobald der A\/B Test in Ihr Projekt eingebunden ist, es sei denn Sie haben ein Startdatum terminiert. Stellen Sie sicher alle benötigten Konfigurationen vorab zu machen, da es nicht empfohlen ist, einen A\/B Test zu verändern, sobald es gestartet wurde.",
        "Rule": "Regel",
        "RunExperimentWithJsClient": "Ausführen eines A\/B Tests im Browser mit dem Matomo JavaScript Tracker",
        "RunExperimentWithJsTracker": "Ausführen eines A\/B Tests für eine Website auf dem Server",
        "RunExperimentWithOtherSDK": "Ausführen eines A\/B Tests in einer Anwendung auf iOS, Android, PHP, Java, C#, Python, ...",
        "RunExperimentWithEmailCampaign": "Ausführen eines A\/B Tests in einer Kampagne (z.B. einer Werbe-Kampagne oder E-Mail-Kampagne)",
        "StatusRunning": "Im Gange",
        "StatusArchived": "Archiviert",
        "StatusFinished": "Beendet",
        "StatusCreated": "Erstellt",
        "StatusActive": "Aktiv",
        "Status": "Status",
        "StartDate": "Startdatum",
        "FinishDate": "Enddatum",
        "Xhours": "%1$s Stunden",
        "1Hour": "1 Stunde",
        "1DayAnd1Hour": "1 Tag und 1 Stunde",
        "1DayAndYHours": "1 Tag and %1$s Stunden",
        "XDaysAnd1Hour": "%1$s Tage und 1 Stunde",
        "XDaysAndYHours": "%1$s Tage und %2$s Stunden",
        "NoActiveExperimentConfigured": "Es stehen keine aktiven A\/B Tests zur Verfügung.",
        "GettingStarted": "Erste Schritte",
        "ExperimentIsFinishedPleaseRemoveCode": "Da der A\/B Test beendet wurde, stellen Sie sicher jeglichen Code für diesen A\/B Test von Ihrer Website, Anwendung oder Server zu entfernen.",
        "Redirects": "Umleitungen",
        "FieldExperimentNameHelp": "Der Name ist ein eindeutiger Name für diesen A\/B Test. Der gewählte Name ist unter Umständen im Quellcode Ihres Projektes für Ihre Besucher sichtbar, wenn der A\/B Test im Gange ist. Verwenden Sie lediglich Buchstaben und Zahlen ohne Leerzeichen oder Sonderzeichen. Maximal %1$s Zeichen sind erlaubt.",
        "FieldHypothesisHelp": "Die Hypothese definiert was Sie von diesem A\/B Test erwarten, was das Ergebnis sein wird und warum es das Ergebnis sein wird. Zum Beispiel \"%1$sWenn%2$s ich die Farbe des Jetzt Kaufen Buttons ändere, %3$sdann%4$s hoffe ich mehr Produkte zu verkaufen, %5$sweil%6$s der Button besser sichtbar sein wird.\". Die Hypothese ist ein wichtiger Schritt in der Definierung Ihres Experiments und es ist empfohlen sich die Zeit zu nehmen darüber nachzudenken.",
        "FieldHypothesisPlaceholder": "z.B. ' Wenn ich die Farbe des Jetzt Kaufen Buttons ändere, dann hoffe ich mehr Produkte zu verkaufen, weil der Button besser sichtbar sein wird.'",
        "FieldDescriptionHelp": "Dieses Feld kann verwendet werden, um zu Beschreiben welche Dinge Sie in diesem A\/B Test miteinander vergleichen. Zum Beispiel \"Vergleiche einen blauen mit einem roten Jetzt Kaufen Button farbe\".",
        "FieldDescriptionPlaceholder": "z.B. 'Vergleiche einen blauen mit einem roten Jetzt Kaufen Button farbe'",
        "ActivateExperimentOnAllPages": "Besucher nehmen an diesem A\/B Test teil, wenn diese irgendeine Seite besuchen",
        "ActiveExperimentOnSomePages": "Besucher nehmen nur an diesem A\/B Test teil, wenn die URL entspricht",
        "NavigationBack": "Zurück",
        "Schedule": "Terminieren",
        "EmbedCode": "Code einbauen",
        "Definition": "Definition",
        "UpdatingData": "Aktualisiere Daten...",
        "FormCreateExperimentIntro": "Ein A\/B Test ermöglicht es Ihnen, verschiedene Versionen miteinander zu vergleichen, um zu sehen, welche am Besten funktioniert. Diese Felder werden benötigt, um einen A\/B Test zu erstellen. Sobald der A\/B Test erstellt wurde, können Sie es weiter an Ihre Bedürfnisse anpassen.",
        "FieldConfidenceThresholdHelp": "Das Ziel eines A\/B Tests ist es, genug Daten zu sammeln um zuversichtlich Änderungen aufgrund des Ergebnisses des A\/B Tests machen zu können. Je höher die ausgewählte Nummer, desto wahrscheinlicher ist es, dass das Ergebnis echt und wiederholbar ist und nicht zufällig erzielt wurde.",
        "FieldMinimumDetectableEffectHelp1": "Die Minimum Erkannte Auswirkung ist die relative minimale Verbesserung die Sie von diesem A\/B Test erwarten. Zum Beispiel, wenn die Konversionsrate eines Ziels derzeit bei 10% liegt und Sie eine Auswirkung von 20% erwarten, dann muss eine Variante mindestens eine Konversionsrate von 12% vorweisen, um als siegreiche Variante zu gelten.",
        "FieldMinimumDetectableEffectHelp2": "Falls Sie durch diesen A\/B Test eine kleine Auswirkung erwarten, ist es empfohlen 10% auszuwählen, für eine mittlere Auswirkung 40% und für eine große Auswirkung 70%.",
        "FieldSuccessConditionsHelp": "Wir verwenden die Minimum Erkannte Auswirkung und die Vertrauensschwelle (Statistische Signifikanz) um die Anzahl der Besucher zu berechnen, die benötigt werden, bevor mit Zuversicht ein bestandskräftiges Ergebnis gemacht werden kann. Während das Ergebnis im Gange ist, sehen Sie womöglich viele verschiedene potentielle Gewinner die die erwartende Auswirkung erreichen. Allerdings ist es wichtig den A\/B Test lange genug laufen zu lassen, um sicher zu stellen, dass die erzielte Auswirkung nicht zufällig erzielt wurde.",
        "NewExperimentTargetPageHelp": "Standardmäßig wird ein A\/B Test auf allen Ihren Seiten aktiviert. Alternativ können Sie einen A\/B Test nur auf einer bestimmten Seite aktivieren. Falls Sie eine URL angeben, wird der A\/B Test nur auf dieser Seite aktiviert. Sobald Sie den A\/B Test erstellt haben, können Sie weitere Seiten ein- und ausschließen.",
        "NeedHelp": "Hilfe benötigt?",
        "TargetPages": "Ziel-Seiten",
        "TrafficAllocation": "Traffic Zuweisung",
        "ActionViewReport": "Gehe zum Report",
        "ActionFinishExperiment": "Beende A\/B Test",
        "ActionArchiveExperiment": "Archiviere A\/B Test",
        "ActionArchiveExperimentSuccess": "A\/B Test erfolgreich archiviert",
        "ActionEditExperimentAnyway": "Bearbeite der A\/B Test dennoch",
        "ConfirmUpdateStartsExperiment": "Das definierte Startdatum ist in der Vergangenheit. Dies bedeutet, dass der A\/B Test sofort gestartet wird, wenn Sie fortfahren. Wollen Sie den A\/B Test wirklich bereits starten? Es ist empfohlen einen A\/B Test nicht zu verändern, sobald es gestartet wurde da es zu Fehlinterpretationen mit dem Ergebnis kommen könnte.",
        "ConfirmFinishExperiment": "Wollen Sie den A\/B Test wirklich beenden? Es wird nicht möglich sein den A\/B Test erneut zu starten sobald es beendet wurde. Vergessen Sie nicht den Code für diesen A\/B Test von Ihrem Projekt zu entfernen damit Ihre Besucher nicht mehr am A\/B Test teilnehmen.",
        "ExperimentRequiresUpdateBeforeViewEmbedCode": "Sie haben nicht gespeicherte Änderungen für diesen A\/B Test. Bitte speichern Sie zunächst den A\/B Test oder brechen Sie die Bearbeitung ab um den Code für diesen A\/B Test sehen zu können.",
        "NoActiveExperiment": "Es existiert kein A\/B Test der derzeit im Gange oder beendet ist.",
        "HowToGetStartedAdminAccess": "Sie können jetzt %1$seinen neuen A\/B-Test erstellen%2$s.",
        "Preview": "Muster-Report",
        "ExperimentReportPreview": "Hier ist ein Beispiel eines Reportes, welcher angezeigt wird, sobald ein A\/B Test gestartet wurde.",
        "CreateNewExperiment": "Neuen A\/B Test erstellen",
        "CreateNewExperimentNow": "Erstelle nun einen neuen A\/B Test",
        "EditExperiment": "Bearbeite A\/B Test %s",
        "EditThisExperiment": "Bearbeite A\/B Test",
        "ManageExperiments": "A\/B Test-Verwaltung",
        "ManageExperimentsIntroduction": "Ein A\/B Test ermöglicht es Ihnen, verschiedene Versionen einer Website oder Anwendung miteinander zu vergleichen um herauszufinden, welche Version Sie am erfolgreichsten macht. A\/B Tests sind auch bekannt als Experimente oder Split tests. In einem A\/B Test erstellen Sie zwei oder mehrere Varianten für Ihre Besucher und die Variante die besser funktioniert, gewinnt. Wenn ein Besucher an einem A\/B Test teilnimmt, wird eine der verschiedenen Varianten zufällig ausgewählt und der Besucher wird diese ausgewählte Variante für alle nachfolgenden Besuche sehen. Wenn Sie so experimentieren, maximieren Sie Ihren Erfolg.",
        "MenuTitleExperiment": "A\/B Test \"%1$s\"",
        "ExperimentCreated": "der A\/B Test wurde erfolgreich erstellt. Nun können Sie es weiter konfigurieren. Es ist empfohlen einen Blick auf die \"Erfolgsmetriken\" zu werfen.",
        "ExperimentUpdated": "der A\/B Test wurde erfolgreich aktualisiert.",
        "ExperimentStarted": "der A\/B Test wurde erfolgreich gestartet.",
        "ExperimentFinished": "der A\/B Test wurde erfolgreich beendet.",
        "Hypothesis": "Hypothese",
        "MinimumDetectableEffectMDE": "Minimum Erkannte Auswirkung",
        "ExpectedImprovement": "Erwartete minimale erkannte Auswirkung",
        "ConfidenceThreshold": "Vertrauensschwelle",
        "ReportStatusRunning": "der A\/B Test ist im Gange für %1$s seit dem %2$s.",
        "ReportStatusFinished": "der A\/B Test wurde beendet. Es lief für %1$s von %2$s bis %3$s.",
        "ReportWhenToDeclareWinner": "Wann kann ein Gewinner ausgemacht werden? Ein A\/B Test deutet möglicherweise eine gewinnende oder verlierende Variante an. Es ist jedoch entscheidend den A\/B Test zumindest für ein oder zwei volle Geschäftsperioden laufen zu lassen. Das Verhalten von Benutzern verändert sich abhängig von der Tageszeit und dem Wochentag und es ist daher empfohlen A\/B Tests für volle Tage oder besser volle Wochen laufen zu lassen, damit die gemessene Auswirkung nicht zufällig erzielt wurde.",
        "ReportDateCannotBeChanged": "Das Datum für einen A\/B Test-Report kann nicht verändert werden",
        "TargetTypeIsAny": "ist beliebig",
        "TargetTypeIsNot": "nicht %s",
        "TargetTypeEqualsExactly": "entspricht genau",
        "TargetTypeEqualsExactlyInfo": "Der Wert muss genau übereinstimmen, inklusive des URL-Schemas, der URL-Parameter und des URL-Hashes.",
        "TargetTypeEqualsSimple": "entspricht einfach",
        "TargetTypeEqualsSimpleInfo": "Das URL-Schema (z.B. http and https) und die subdomain \"www.\" muss nicht übereinstimmen und wird ignoriert. Ein abschließender Schrägstrich im Pfad wie auch URL-Parameter und URL-Hashes werden ebenso ignoriert wenn die URL verglichen wird.",
        "TargetTypeContains": "enthält",
        "TargetTypeExists": "existiert",
        "TargetTypeStartsWith": "startet mit",
        "TargetTypeRegExp": "entspricht dem Regulären Ausdruck",
        "TargetTypeRegExpInfo": "Beliebiger Regulärer Ausdruck, zum Beispiel \"^(.*)test(.*)$\".",
        "TargetComparisionsCaseInsensitive": "Alle Vergleiche ignorieren Groß- und Kleinschreibung.",
        "TargetComparisons": "Vergleiche",
        "TargetAttributeUrl": "URL",
        "TargetAttributePath": "Pfad",
        "FilesystemDirectory": "verzeichnis",
        "TargetAttributeUrlParameter": "URL Parameter",
        "TargetAttributeUrlParameterExample": "nameEinesUrlParameters",
        "Manage": "Verwalten",
        "Experiment": "A\/B Test",
        "Experiments": "A\/B Tests",
        "ExperimentName": "A\/B Test-Name",
        "ExperimentOverview": "A\/B Test Übersicht",
        "Variation": "Variante",
        "Variations": "Varianten",
        "VariationName": "Varianten-Name",
        "VariationRedirectUrl": "Varianten-URL",
        "VariationPercentage": "Variation percentage",
        "PercentageParticipants": "Percentage participants",
        "SuccessConditions": "Erfolgsbedingungen",
        "SuccessMetric": "Erfolgsmetrik",
        "SuccessMetrics": "Erfolgsmetriken",
        "SuccessMetricDetails": "Erfolgsmetrik details",
        "Target": "Ziel",
        "IncludedTargets": "Inbegriffenes Ziel",
        "ExcludedTargets": "Ausgeschlossenes Ziel",
        "AverageX": "Durchschnitt %1$s",
        "VisitEnteredExperiment": "Besuch hat am A\/B Test teil genommen",
        "VisitorEnteredNExperiments": "Der Besucher hat %1$s A\/B-Tests durchlaufen",
        "VisitorEnteredOneExperiment": "Der Besucher hat einen A\/B-Test durchlaufen",
        "VisitsActivelyEntered": "Besuch aktiv teil genommen",
        "UniqueVisitorsActivelyEntered": "Eindeutige Besucher aktiv teil genommen"
    }
}